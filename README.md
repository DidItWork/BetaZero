# BetaZero
An implementation of Reinforcement Learning in game-playing according to Alpha Zero

## Logic

### Game Tree Node Attributes
- $N_i$, number of times node has been selected / number of times the node has been through the simulation (integer)
- $W_i$, the sum of expected value of the node (not an integer, "the number of wins for the node")
- $p$, policy values of child nodes
- $s$, representation of board state (8x8xN tensor)

### Alpha Zero MCTS
1. Selection: Start from root node (current game state) and select successive nodes based on Upper Confidence Bound Criterion (UCB) until a leaf node L is reached (a leaf node is any node that has a potential child from which no simulation has yet been initiated) or a terminal node.
$$\text{UCB} = \frac{W_i}{N_i}+p_ic\frac{\sqrt{N_i}}{1+n_i}$$
, where $p_i$ is the policy of the child node and $n_i$ is its simulation count
3. Expansion: Unless L ends the game decisively for either player, randomly initialize an unexplored child node.
4. Backpropagation: Using the value generated by the neural network $f_\theta$, update the N and W values of the current node and all its parent nodes.
5. Repeat steps 1 to 3 for N iterations

### Self-Play and Training
1. Self-Play until the game ends using MCTS and $f_\theta$
2. Store the chosen action taken at each state and the values of the node (-1,0,1) depending on the player and whether he won or lost the game. One training sample should contain: (board state s, the action chosen $\pi$, the value of the node z)
3. Minimize loss function of the training samples in the batch.
$$l = (z-v)^2-\pi^T\log{p}+c||\theta||^2$$
